LibriSpeech
Vowel Peak detection performance

GPR constrained in the range (200, 300)
	RFC Pre Trained is not trained using filterbank energy
		rfc_pre_trained.score(test_vf[:,:7], np.ones((len(test_vf),1)))
		Out[19]: 0.808575176258403

		rfc_pre_trained.score(test_cf[:,:7], -1*np.ones((len(test_cf),1)))
		Out[20]: 0.7602208492178257

	RFC is the model trained using filterbank energy features too.
		rfc.score(test_vf, np.ones((len(test_vf),1)))
		Out[17]: 0.8170191834727004

		rfc.score(test_cf, -1*np.ones((len(test_cf),1)))
		Out[18]: 0.7799395293808334

	(100,2) configuration of the neural network
		mlp.score(scaled_test_vf, np.ones((len(scaled_test_vf),1)))
		Out[80]: 0.8452205279554025

		mlp.score(scaled_test_cf, -1*np.ones((len(scaled_test_cf),1)))
		Out[81]: 0.7537136847640331

GPR constrained in the range (100, 500)
	RFC is the model trained using filterbank enrgy features too
		rfc.score(test_vf, np.ones((len(test_vf),1)))
		Out[5]: 0.8485863384998832

		rfc.score(test_cf, -1*np.ones((len(test_cf),1)))
		Out[6]: 0.7699092419787805

		('mermel    :100', 8489, 1605)
		('villing   :25', 8393, 1352)
		('peak det  :(2400, 1700)', 7680, 1555)

	(100, 2) configuration of the neural network
		mlp.score(scaled_test_vf, np.ones((len(scaled_test_vf),1)))
		Out[10]: 0.8749902640392554

		mlp.score(scaled_test_cf, -1*np.ones((len(scaled_test_cf),1)))
		Out[11]: 0.7410200690272274

		('mermel    :100', 8489, 1605)
		('villing   :25', 8393, 1352)
		('peak det  :(2400, 1700)', 7709, 1607)

GPR constrained in range (150, 350)
	RFC is trained using filterbank energy and VUV features
		rfc.score(test_vf, np.ones((len(test_vf),1)))
		Out[10]: 0.7793661808928984

		rfc.score(test_cf, -1*np.ones((len(test_cf),1)))
		Out[11]: 0.8335782512858193

	(100, 2) configuration of Neural Net trained using VUV features
		mlp.score(scaled_test_vf, np.ones((len(scaled_test_vf),1)))
		Out[13]: 0.7610918343742771

		mlp.score(scaled_test_cf, -1*np.ones((len(scaled_test_cf),1)))
		Out[14]: 0.799746175940151


	(depth=4, trees=40) config of Random Forest with Filterbank and VUV features
		('mermel    :100', 8489, 1605)
		('villing   :25', 8393, 1352)                        # Best so far
		('peak det  :(2400, 1700)', 7592, 1477)

	(70,50,2) config of Neural Net with VUV features
		('mermel    :100', 8489, 1605)
		('villing   :25', 8393, 1352)
		('peak det  :(2400, 1700)', 7854, 1664)

	TER : Mel-0.33, Vil-0.32, Peak-0.40








TIMIT
Vowel Peak Detection performance

GPR constrained to be in range (200, 300)
	RFC Pre Trained is not trained using filterbank energy
		rfc_pre_trained.score(test_vf[:,:7], np.ones((len(test_vf),1)))
		Out[11]: 0.9150208371081717

		rfc_pre_trained.score(test_cf[:,:7], -1*np.ones((len(test_cf),1)))
		Out[12]: 0.7536894923258559

	RFC is the model trained using filterbank energy features too.
		rfc.score(test_vf, np.ones((len(test_vf),1)))
		Out[8]: 0.9300597934408408

		rfc.score(test_cf, -1*np.ones((len(test_cf),1)))
		Out[9]: 0.7479338842975206

	(100, 2) configuration of the neural network
		mlp.score(scaled_test_vf, np.ones((len(scaled_test_vf),1)))
		Out[17]: 0.9313281391556442

		mlp.score(scaled_test_cf, -1*np.ones((len(scaled_test_cf),1)))
		Out[18]: 0.7238783943329398


GPR constrained to be in range (100, 500)
	RFC is the model trained using filterbank energy features too.
		rfc.score(test_vf, np.ones((len(test_vf),1)))
		Out[31]: 0.930547494679822

		rfc.score(test_cf, -1*np.ones((len(test_cf),1)))
		Out[32]: 0.7649358773850485

	(100, 2) configuration of the neural network
		mlp.score(scaled_test_vf, np.ones((len(test_vf),1)))
		Out[29]: 0.93905977945444

		mlp.score(scaled_test_cf, np.zeros((len(test_cf),1)))
		Out[30]: 0.7632155145448858


GPR constrained in range (150, 350)
	RFC is trained using filterbank energy and VUV features
		rfc.score(test_vf, np.ones((len(test_vf),1)))
		Out[139]: 0.9304989322461658

		rfc.score(test_cf, -1*np.ones((len(test_cf),1)))
		Out[140]: 0.7636650868878357

	(100, 2) configuration of Neural Net trained using VUV features
		mlp.score(scaled_test_vf, np.ones((len(test_vf),1)))
		Out[137]: 0.9332168510968744

		mlp.score(scaled_test_cf, -1*np.ones((len(test_cf),1)))
		Out[138]: 0.7693522906793049

	(depth=9, trees=30) config of Random Forest with Filterbank and VUV features
		('mermel    :100', 4037, 808)
		('villing   :25', 3926, 704)               
		('peak det  :(2400, 1700)', 3978, 646)

	(80,60,2) config of Neural Net with VUV features
		('mermel    :100', 4037, 808)
		('villing   :25', 3926, 704)				# Best so far using MLP
		('peak det  :(2400, 1700)', 3983, 648)
	
	TER : Mel-0.363, Vil-0.361, Peak-0.342















Hyper Parameter Optimization TIMIT
	Random Forest (depth, num_trees, true_detection, false_alarms)
		(3, 10, 0.9143855562026791, 0.23317535545023696)
		(3, 20, 0.9054552514074937, 0.22496050552922592)
		(3, 30, 0.9110852261696758, 0.23096366508688784)
		(3, 40, 0.9075907590759076, 0.23033175355450236)
		(3, 50, 0.9077848961366725, 0.2287519747235387)
		(3, 60, 0.9095321296835566, 0.2292259083728278)
		(3, 70, 0.9091438555620268, 0.22985781990521326)
		(3, 80, 0.90293146961755, 0.225434439178515)
		(3, 90, 0.9099204038050864, 0.2282780410742496)
		(5, 10, 0.9200155309648612, 0.23238546603475513)
		(5, 20, 0.9271986022131625, 0.23854660347551343)
		(5, 30, 0.9237041351193943, 0.23522906793048973)
		(5, 40, 0.9215686274509803, 0.23285939968404423)
		(5, 50, 0.9229275868763347, 0.23428120063191155)
		(5, 60, 0.92234517569404, 0.2344391785150079)
		(5, 70, 0.9242865463016889, 0.23428120063191155)
		(5, 80, 0.9250630945447486, 0.234913112164297)
		(5, 90, 0.9270044651523975, 0.23759873617693522)
		(7, 10, 0.9326344399145797, 0.23933649289099526)
		(7, 20, 0.9295282469423413, 0.23712480252764612)
		(7, 30, 0.9297223840031061, 0.23412322274881517)
		(7, 40, 0.9289458357600466, 0.2344391785150079)
		(7, 50, 0.9283634245777519, 0.23633491311216429)
		(7, 60, 0.9299165210638711, 0.23554502369668245)
		(7, 70, 0.9295282469423413, 0.234913112164297)
		(7, 80, 0.9299165210638711, 0.23854660347551343)
		(7, 90, 0.930110658124636, 0.23712480252764612)
		(9, 10, 0.9341875364006988, 0.23617693522906794)
		(9, 20, 0.9322461657930499, 0.2339652448657188)
		(9, 30, 0.9361289070083479, 0.23507109004739338)     # chosen for Random Forest
		(9, 40, 0.9330227140361095, 0.2358609794628752)
		(9, 50, 0.9312754804892254, 0.23475513428120062)
		(9, 60, 0.9341875364006988, 0.23412322274881517)
		(9, 70, 0.9336051252184042, 0.234913112164297)
		(9, 80, 0.9345758105222287, 0.23601895734597156)
		(9, 90, 0.9349640846437585, 0.2358609794628752)
		(11, 10, 0.9279751504562221, 0.23033175355450236)
		(11, 20, 0.9347699475829936, 0.23285939968404423)
		(11, 30, 0.933993399339934, 0.234913112164297)
		(11, 40, 0.9357406328868181, 0.23475513428120062)
		(11, 50, 0.9355464958260532, 0.23285939968404423)
		(11, 60, 0.935934769947583, 0.23554502369668245)
		(11, 70, 0.935934769947583, 0.2353870458135861)
		(11, 80, 0.9349640846437585, 0.2339652448657188)
		(11, 90, 0.9355464958260532, 0.23270142180094786)
		(13, 10, 0.9279751504562221, 0.22985781990521326)
		(13, 20, 0.9324403028538147, 0.23127962085308057)
		(13, 30, 0.9326344399145797, 0.23238546603475513)
		(13, 40, 0.9336051252184042, 0.23285939968404423)
		(13, 50, 0.9334109881576393, 0.23048973143759874)
		(13, 60, 0.9351582217045233, 0.23080568720379147)
		(13, 70, 0.9337992622791691, 0.23001579778830963)
		(13, 80, 0.9341875364006988, 0.23064770932069512)
		(13, 90, 0.9355464958260532, 0.23428120063191155)
		(15, 10, 0.920403805086391, 0.2244865718799368)
		(15, 20, 0.9295282469423413, 0.22859399684044235)
		(15, 30, 0.9308872063676956, 0.22843601895734597)
		(15, 40, 0.9326344399145797, 0.2292259083728278)
		(15, 50, 0.9326344399145797, 0.22559241706161137)
		(15, 60, 0.9347699475829936, 0.22859399684044235)
		(15, 70, 0.9334109881576393, 0.2282780410742496)
		(15, 80, 0.9336051252184042, 0.22985781990521326)
		(15, 90, 0.9353523587652883, 0.22985781990521326)
		(17, 10, 0.9097262667443214, 0.22053712480252766)
		(17, 20, 0.9256455057270433, 0.22622432859399685)
		(17, 30, 0.9271986022131625, 0.22780410742496052)
		(17, 40, 0.9312754804892254, 0.22890995260663508)
		(17, 50, 0.9318578916715201, 0.22764612954186414)
		(17, 60, 0.9314696175499902, 0.2268562401263823)
		(17, 70, 0.9322461657930499, 0.2273301737756714)
		(17, 80, 0.9337992622791691, 0.2259083728278041)
		(17, 90, 0.9328285769753446, 0.22669826224328593)
		(19, 10, 0.9108910891089109, 0.22037914691943128)
		(19, 20, 0.9229275868763347, 0.22180094786729856)
		(19, 30, 0.926033779848573, 0.22606635071090048)
		(19, 40, 0.9306930693069307, 0.225434439178515)
		(19, 50, 0.9279751504562221, 0.22575039494470775)
		(19, 60, 0.9314696175499902, 0.225434439178515)
		(19, 70, 0.9310813434284605, 0.2273301737756714)
		(19, 80, 0.9312754804892254, 0.2268562401263823)
		(19, 90, 0.932052028732285, 0.22575039494470775)


	MLP (nodes_hidden_layer1, nodes_hidden_layer2, true_detection, false_alarms)
		(10, 0, 0.9225393127548049, 0.22575039494470775)
		(10, 20, 0.9289458357600466, 0.23317535545023696)
		(10, 40, 0.9277810133954572, 0.23380726698262244)
		(10, 60, 0.9287516986992816, 0.23412322274881517)
		(10, 80, 0.9268103280916327, 0.230173775671406)
		(20, 0, 0.9240924092409241, 0.22938388625592418)
		(20, 20, 0.9273927392739274, 0.22748815165876776)
		(20, 40, 0.9266161910308678, 0.2282780410742496)
		(20, 60, 0.9246748204232188, 0.22622432859399685)
		(20, 80, 0.9273927392739274, 0.23127962085308057)
		(30, 0, 0.9248689574839837, 0.2259083728278041)
		(30, 20, 0.9258396427878082, 0.22938388625592418)
		(30, 40, 0.9332168510968744, 0.2358609794628752)
		(30, 60, 0.9341875364006988, 0.2339652448657188)
		(30, 80, 0.9221510386332751, 0.22575039494470775)
		(40, 0, 0.9312754804892254, 0.23191153238546602)
		(40, 20, 0.9279751504562221, 0.2268562401263823)
		(40, 40, 0.9303047951854009, 0.22796208530805687)
		(40, 60, 0.9213744903902155, 0.22527646129541865)
		(40, 80, 0.9225393127548049, 0.22480252764612954)
		(50, 0, 0.9163269268103281, 0.22511848341232227)
		(50, 20, 0.9270044651523975, 0.2259083728278041)
		(50, 40, 0.9238982721801592, 0.22717219589257504)
		(50, 60, 0.9209862162686857, 0.22796208530805687)
		(50, 80, 0.0, 0.0)
		(60, 0, 0.9172976121141526, 0.225434439178515)
		(60, 20, 0.928169287516987, 0.2263823064770932)
		(60, 40, 0.9215686274509803, 0.2263823064770932)
		(60, 60, 0.9273927392739274, 0.22654028436018958)
		(60, 80, 0.9312754804892254, 0.22843601895734597)
		(70, 0, 0.9310813434284605, 0.22812006319115324)
		(70, 20, 0.9316637546107552, 0.23033175355450236)
		(70, 40, 0.932052028732285, 0.22843601895734597)
		(70, 60, 0.9355464958260532, 0.23270142180094786)
		(70, 80, 0.9264220539701029, 0.2287519747235387)
		(80, 0, 0.9273927392739274, 0.23096366508688784)
		(80, 20, 0.9351582217045233, 0.2334913112164297)
		(80, 40, 0.9287516986992816, 0.22369668246445498)
		(80, 60, 0.939623374102116, 0.23633491311216429)      # chosen for MLP
		(80, 80, 0.9390409629198214, 0.23570300157977883)
		(90, 0, 0.9225393127548049, 0.22890995260663508)
		(90, 20, 0.926033779848573, 0.2273301737756714)
		(90, 40, 0.9250630945447486, 0.2282780410742496)
		(90, 60, 0.9287516986992816, 0.2287519747235387)
		(90, 80, 0.920403805086391, 0.2241706161137441)



Hyper-parameter optimization LibriSpeech

	Random Forest (depth, num_trees, true_detection, false_alarms)
		(3, 10, 0.7605366643534582, 0.1920045421147552)
		(3, 20, 0.7885264862364099, 0.2062320486273462)
		(3, 30, 0.7848253527642841, 0.21808830405450538)
		(3, 40, 0.7863058061531344, 0.20626544652995793)
		(3, 50, 0.7899606754568587, 0.21023979694075212)
		(3, 60, 0.7845015035854731, 0.20596486540645248)
		(3, 70, 0.7813092759657645, 0.21117493821388017)
		(3, 80, 0.7907009021512839, 0.20526350945160643)
		(3, 90, 0.7871848253527642, 0.19928528488410927)
		(5, 10, 0.7902382604672681, 0.18769621267784384)
		(5, 20, 0.7760351607679852, 0.17881237058312738)
		(5, 30, 0.7708998380754106, 0.174504041146216)
		(5, 40, 0.7915799213509137, 0.18151760069467637) 	# Chosen for Random Forest
		(5, 50, 0.7805227851029378, 0.1757397635428495)
		(5, 60, 0.7757575757575758, 0.17376928728875826)
		(5, 70, 0.7881101087207958, 0.17911295170663283)
		(5, 80, 0.7795512375665048, 0.17500500968539176)
		(5, 90, 0.7824196160074023, 0.17687529223164786)
		(7, 10, 0.7727966689798751, 0.1684256228708837)
		(7, 20, 0.7726578764746703, 0.16294836684256228)
		(7, 30, 0.7689567430025446, 0.16565359695411128)
		(7, 40, 0.7789035392088827, 0.16892659141005945)
		(7, 50, 0.7807541059449457, 0.1677910627212611)
		(7, 60, 0.7783483691880638, 0.16364972279740833)
		(7, 70, 0.7755262549155679, 0.16555340324627613)
		(7, 80, 0.7766828591256072, 0.16622136129851045)
		(7, 90, 0.7740920656951191, 0.16428428294703093)
		(9, 10, 0.7814943326393708, 0.1721661879633959)
		(9, 20, 0.7764515382835994, 0.16187963395898738)
		(9, 30, 0.7757575757575758, 0.15890722062654467)
		(9, 40, 0.7760351607679852, 0.16328234586867946)
		(9, 50, 0.778764746703678, 0.1583728541847572)
		(9, 60, 0.7759888965995836, 0.16221361298510453)
		(9, 70, 0.7760814249363868, 0.1610446863936945)
		(9, 80, 0.7756650474207726, 0.15997595351011956)
		(9, 90, 0.7726578764746703, 0.1598423618996727)
		(11, 10, 0.768262780476521, 0.16348273328434973)
		(11, 20, 0.7750173490631506, 0.16141206332242336)
		(11, 30, 0.7724728198010641, 0.16264778571905683)
		(11, 40, 0.7804302567661346, 0.16201322556943423)
		(11, 50, 0.7734443673374971, 0.15429831006612785)
		(11, 60, 0.7752024057367569, 0.15653596954111282)
		(11, 70, 0.779088595882489, 0.1587736290160978)
		(11, 80, 0.7770529724728198, 0.15867343530826264)
		(11, 90, 0.7746009715475365, 0.1560683989045488)
		(13, 10, 0.7659033078880407, 0.16648854451940417)
		(13, 20, 0.7656719870460329, 0.15686994856722997)
		(13, 30, 0.7771917649780246, 0.157103733885512)
		(13, 40, 0.7732130464954893, 0.15740431500901744)
		(13, 50, 0.777793199167245, 0.15743771291162914)
		(13, 60, 0.7755725190839695, 0.15827266047692204)
		(13, 70, 0.776590330788804, 0.15656936744372454)
		(13, 80, 0.7763590099467962, 0.15540044085231447)
		(13, 90, 0.7727966689798751, 0.15436510587135127)
		(15, 10, 0.7673374971084895, 0.1640170997261372)
		(15, 20, 0.7668748554244738, 0.15713713178812372)
		(15, 30, 0.7716400647698357, 0.15673635695678312)
		(15, 40, 0.7707610455702059, 0.1545988911896333)
		(15, 50, 0.7748322923895443, 0.15717052969073542)
		(15, 60, 0.7729817256534813, 0.154632289092245)
		(15, 70, 0.7650705528568124, 0.15383073942956382)
		(15, 80, 0.7726578764746703, 0.15329637298777637)
		(15, 90, 0.7758038399259773, 0.15453209538440985)
		(17, 10, 0.7574369650705528, 0.1598423618996727)
		(17, 20, 0.7637751561415683, 0.15843964998998064)
		(17, 30, 0.774323386537127, 0.15720392759334714)
		(17, 40, 0.7716863289382373, 0.15790528354819316)
		(17, 50, 0.7717325931066389, 0.15176006946763743)
		(17, 60, 0.7649317603516077, 0.15219424220158975)
		(17, 70, 0.7697894980337728, 0.154632289092245)
		(17, 80, 0.7741845940319223, 0.15383073942956382)
		(17, 90, 0.7669673837612769, 0.15082492819450938)
		(19, 10, 0.740735600277585, 0.1565025716385011)
		(19, 20, 0.7623409669211196, 0.15616859261238394)
		(19, 30, 0.7635900994679621, 0.15216084429897803)
		(19, 40, 0.7614156835530881, 0.1508583260971211)
		(19, 50, 0.7681702521397178, 0.15289559815643577)
		(19, 60, 0.7716863289382373, 0.15413132055306927)
		(19, 70, 0.769234328012954, 0.15269521074076547)
		(19, 80, 0.77117742308582, 0.1533631687929998)
		(19, 90, 0.7713624797594263, 0.1529289960590475)
		

	MLP (nodes_hidden_layer1, nodes_hidden_layer2, true_detection, false_alarms)
		(10, 0, 0.756234096692112, 0.2567630752788725)
		(10, 10, 0.7527180198935924, 0.20322623739229176)
		(10, 20, 0.760999306037474, 0.22069334045821923)
		(10, 30, 0.7612306268794818, 0.24771224367109745)
		(10, 40, 0.7630349294471432, 0.19190434840692006)
		(10, 50, 0.0, 0.0)
		(10, 60, 0.744529262086514, 0.19497695544719792)
		(10, 70, 0.0, 0.0)
		(10, 80, 0.7659958362248439, 0.2055974884777236)
		(10, 90, 0.7647004395095998, 0.22139469641306525)
		(20, 0, 0.7666435345824659, 0.22192906285485273)
		(20, 10, 0.7282905389775619, 0.19387482466101127)
		(20, 20, 0.7279204256303493, 0.21087435709037472)
		(20, 30, 0.757205644228545, 0.19220492953042548)
		(20, 40, 0.7104788341429563, 0.1650858326097121)
		(20, 50, 0.7690955355077492, 0.24644312337185226)
		(20, 60, 0.7487393014110572, 0.22096052367911295)
		(20, 70, 0.7317603516076798, 0.18271992518869815)
		(20, 80, 0.752116585704372, 0.20459555139937213)
		(20, 90, 0.7383761276891048, 0.19751519604568832)
		(30, 0, 0.7660421003932454, 0.2164184089239196)
		(30, 10, 0.761323155216285, 0.213078618662748)
		(30, 20, 0.7727504048114735, 0.20760136263442655)
		(30, 30, 0.0, 0.0)
		(30, 40, 0.7403192227619708, 0.18064925522677175)
		(30, 50, 0.7402266944251678, 0.20806893327099057)
		(30, 60, 0.7802452000925283, 0.2029256562687863)
		(30, 70, 0.7352301642377979, 0.17861198316745708)
		(30, 80, 0.7335183900069396, 0.20302584997662146)
		(30, 90, 0.7395327318991441, 0.1778104335047759)
		(40, 0, 0.760999306037474, 0.23645715049094918)
		(40, 10, 0.7616007402266944, 0.1989179079553804)
		(40, 20, 0.7618783252371039, 0.2308797007547926)
		(40, 30, 0.7202868378440898, 0.19073542181550998)
		(40, 40, 0.7585473051121906, 0.20285886046356288)
		(40, 50, 0.7379597501734906, 0.2039609912497495)
		(40, 60, 0.7448068470969235, 0.19794936877964064)
		(40, 70, 0.0, 0.0)
		(40, 80, 0.7496645847790886, 0.20052100728074276)
		(40, 90, 0.7498959056210964, 0.21598423618996726)
		(50, 0, 0.7585935692805922, 0.2365239462961726)
		(50, 10, 0.7356928059218135, 0.22930999933204194)
		(50, 20, 0.7433726578764747, 0.19861732683187497)
		(50, 30, 0.7715475364330326, 0.19574510720726737)
		(50, 40, 0.7678001387925052, 0.19447598690802217)
		(50, 50, 0.7500809622947028, 0.21825529356756396)
		(50, 60, 0.7366643534582465, 0.2142141473515463)
		(50, 70, 0.7481378672218367, 0.19668024848039542)
		(50, 80, 0.0, 0.0)
		(50, 90, 0.7451306962757345, 0.18415603500100194)
		(60, 0, 0.7688179504973398, 0.21064057177209272)
		(60, 10, 0.760351607679852, 0.21989179079553803)
		(60, 20, 0.7555401341660883, 0.2058312737960056)
		(60, 30, 0.7358778625954199, 0.1957785051098791)
		(60, 40, 0.7354152209114041, 0.1908022176207334)
		(60, 50, 0.0, 0.0)
		(60, 60, 0.7176497802452001, 0.19818315409792264)
		(60, 70, 0.7294471431876012, 0.20723398570569768)
		(60, 80, 0.7337959750173491, 0.17159842361899671)
		(60, 90, 0.7300948415452232, 0.19330706031661213)
		(70, 0, 0.768031459634513, 0.19587869881771425)
		(70, 10, 0.7671061762664816, 0.23121367978090976)
		(70, 20, 0.7574832292389544, 0.20048760937813107)
		(70, 30, 0.7354614850798057, 0.1911695945494623)
		(70, 40, 0.7529956049040019, 0.22079353416605438)
		(70, 50, 0.7808466342817488, 0.22827466435107877) 	# Chosen for MLP
		(70, 60, 0.7363405042794355, 0.18271992518869815)
		(70, 70, 0.7280129539671525, 0.1861265112550932)
		(70, 80, 0.7356002775850105, 0.20776835214748512)
		(70, 90, 0.7326393708073098, 0.2026250751452809)
		(80, 0, 0.7291695581771918, 0.19240531694609578)
		(80, 10, 0.7395789960675457, 0.19761538975352347)
		(80, 20, 0.7703909322229933, 0.22306459154365105)
		(80, 30, 0.7192227619708536, 0.2091710640571772)
		(80, 40, 0.7342123525329632, 0.2058312737960056)
		(80, 50, 0.7189914411288457, 0.18038207200587802)
		(80, 60, 0.7325468424705066, 0.19788257297441722)
		(80, 70, 0.7481378672218367, 0.19374123305056443)
		(80, 80, 0.7367106176266481, 0.21097455079820987)
		(80, 90, 0.7275503122831367, 0.17827800414133993)
		(90, 0, 0.7494795281054824, 0.19798276668225234)
		(90, 10, 0.7468887346749942, 0.19517734286286822)
		(90, 20, 0.7520240573675688, 0.20462894930198383)
		(90, 30, 0.7433726578764747, 0.23204862734620266)
		(90, 40, 0.7376359009946796, 0.20743437312136798)
		(90, 50, 0.7526254915567893, 0.19427559949235187)
		(90, 60, 0.7542447374508443, 0.193908222563623)
		(90, 70, 0.7329632199861208, 0.1825863335782513)
		(90, 80, 0.7337959750173491, 0.18736223365172666)
		(90, 90, 0.7383761276891048, 0.182385946162581)